{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "phone_pattern = re.compile(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}')\n",
    "url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n",
    "contact_pattern = re.compile(r'contact', re.IGNORECASE)\n",
    "about_pattern = re.compile(r'about', re.IGNORECASE)\n",
    "\n",
    "def clean_url(url):\n",
    "    if url:\n",
    "        # Remove trailing dots or invalid characters\n",
    "        url = url.strip().strip('.')\n",
    "        # Validate the URL format\n",
    "        if url_pattern.match(url):\n",
    "            return url\n",
    "    return None\n",
    "\n",
    "def remove_symbols(input_string):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', input_string).replace('\\n', ' ').lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.replace(' ', '') \n",
    "\n",
    "def remove_letters(input_string):\n",
    "    return re.sub(r'[A-Za-z]', '', input_string)\n",
    "\n",
    "\n",
    "def findEmails(soup, emails):\n",
    "    email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "\n",
    "    email = soup.find_all(string=email_pattern)\n",
    "    for mail in email:\n",
    "        if mail.get_text() not in emails:\n",
    "            emails.append(mail.get_text())\n",
    "\n",
    "def findPhoneNumbers(soup, phoneNumbers):   \n",
    "    phone_pattern = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "\n",
    "    # Find phone numbers\n",
    "    phone_numbers = soup.find_all(string=phone_pattern)\n",
    "    for phone in phone_numbers:                    \n",
    "        phone_text = remove_symbols(phone.get_text(strip=True))\n",
    "        if phone_text not in phoneNumbers:\n",
    "            phoneNumbers.append(phone_text)\n",
    "\n",
    "    # Find phone numbers in links\n",
    "    links = soup.find_all('a', href=True)\n",
    "    for link in links:\n",
    "        if phone_pattern.search(link['href']):\n",
    "            phone_text = remove_symbols(phone_pattern.search(link['href']).group())\n",
    "            if phone_text not in phoneNumbers:\n",
    "                phoneNumbers.append(phone_text)\n",
    "    \n",
    "    if phoneNumbers is not None and len(phoneNumbers) > 0:\n",
    "        for index in range(len(phoneNumbers)):\n",
    "            phoneNumbers[index] = remove_letters(phoneNumbers[index])\n",
    "            phoneNumbers[index] = remove_symbols(phoneNumbers[index])\n",
    "\n",
    "            if len(phoneNumbers[index]) == 10: \n",
    "                phoneNumbers[index] = f\"({phoneNumbers[index][:3]}) {phoneNumbers[index][3:6]}-{phoneNumbers[index][6:]}\"\n",
    "            elif len(phoneNumbers[index]) == 7:\n",
    "                phoneNumbers[index] = f\"{phoneNumbers[index][:3]}-{phoneNumbers[index][3:]}\"\n",
    "            elif 11 <= len(phoneNumbers[index]) <= 15:  # International numbers\n",
    "                phoneNumbers[index] = f\"+{phoneNumbers[index][0]} {phoneNumbers[index][1:4]} {phoneNumbers[index][4:7]}-{phoneNumbers[index][7:]}\"            \n",
    "                \n",
    "def findLinks(soup, hrefs):\n",
    "    social_media_links = soup.find_all('a', href=True)\n",
    "    for link in social_media_links:\n",
    "        href = link['href']\n",
    "        if any(social in href for social in ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com']):\n",
    "            if href not in hrefs:\n",
    "                hrefs.append(href)\n",
    "\n",
    "def findText(soup, addressSites):\n",
    "    address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:Street|St|Avenue|Ave|Boulevard|Blvd|Road|Rd|Lane|Ln|Drive|Dr|Court|Ct|Circle|Cir|Way|Place|Pl|Square|Sq|Trail|Trl|Parkway|Pkwy|Commons|Cmns|Loop|Lp|Terrace|Ter|Highway|Hwy|Expressway|Expy|Freeway|Fwy|Turnpike|Tpke|Alley|Aly|Plaza|Plz|Center|Ctr|Mall|Mll|Walk|Wlk|Path|Pth|Row|Rw)')\n",
    "    addresses = soup.find_all(string=address_pattern)\n",
    "    for address in addresses:\n",
    "        if address.get_text(strip=True) not in addressSites:\n",
    "            addressSites.append([address.get_text(strip=True)])\n",
    "        else:\n",
    "            addressSites.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 61: https://baxland.com\n",
      "1\n",
      "1\n",
      "Processing 2 of 61: https://befeni-usa.com\n",
      "2\n",
      "2\n",
      "Processing 3 of 61: https://chugiak.org\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "Processing 4 of 61: https://hangtownkc.org\n",
      "4\n",
      "4\n",
      "Processing 5 of 61: https://myrtlebeach.city\n",
      "5\n",
      "5\n",
      "Processing 6 of 61: https://wordofgodokc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "Processing 7 of 61: https://bigdiamondpools.com\n",
      "7\n",
      "7\n",
      "Processing 8 of 61: https://brynbachman.com\n",
      "8\n",
      "8\n",
      "Processing 9 of 61: https://bytheseat.com\n",
      "9\n",
      "9\n",
      "Processing 10 of 61: https://gentleconfections.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "Processing 11 of 61: https://hairdesigners-hairsalon.business.site\n",
      "11\n",
      "11\n",
      "Processing 12 of 61: https://poocheshouseboutique.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "Processing 13 of 61: https://thermproutah.com\n",
      "13\n",
      "13\n",
      "Processing 14 of 61: https://yttangsoodo.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "Processing 15 of 61: https://barbaranichols.org\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "Processing 16 of 61: https://big-dog-boarding.business.site\n",
      "16\n",
      "16\n",
      "Processing 17 of 61: https://kiddnap.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "Processing 18 of 61: https://kiplearningandactivitycenter.com\n",
      "18\n",
      "18\n",
      "Processing 19 of 61: https://thecheapghostwriter.com\n",
      "19\n",
      "19\n",
      "Processing 20 of 61: https://blacknewengland.org\n",
      "20\n",
      "20\n",
      "Processing 21 of 61: https://drfconaway.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "21\n",
      "Processing 22 of 61: https://flash-wingo.com\n",
      "22\n",
      "22\n",
      "Processing 23 of 61: https://getrazuzz.com\n",
      "23\n",
      "23\n",
      "Processing 24 of 61: https://oneforallartists.com\n",
      "24\n",
      "24\n",
      "Processing 25 of 61: https://sanctuaryfarmphila.org\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "Processing 26 of 61: https://crabhouse39.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n",
      "Processing 27 of 61: https://latinoyouthoutreach.org\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "27\n",
      "Processing 28 of 61: https://reignvolleyball.com\n",
      "28\n",
      "28\n",
      "Processing 29 of 61: https://wibidata.com\n",
      "29\n",
      "29\n",
      "Processing 30 of 61: https://diecutstickers.com\n",
      "30\n",
      "30\n",
      "Processing 31 of 61: https://mbcollegeguidance.com\n",
      "31\n",
      "31\n",
      "Processing 32 of 61: https://thecollectiveeffect.org\n",
      "32\n",
      "32\n",
      "Processing 33 of 61: https://dentaltalentnow.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n",
      "Processing 34 of 61: https://edgeinsurance.biz\n",
      "34\n",
      "34\n",
      "Processing 35 of 61: https://awlsnap.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n",
      "Processing 36 of 61: https://bgkinvestments.com\n",
      "36\n",
      "36\n",
      "Processing 37 of 61: https://haciendailusionpr.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "37\n",
      "Processing 38 of 61: https://hawaii432.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "38\n",
      "Processing 39 of 61: https://usconstructgroup.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "39\n",
      "Processing 40 of 61: https://wheeinc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "Processing 41 of 61: https://dafacargo.com\n",
      "41\n",
      "41\n",
      "Processing 42 of 61: https://mahoganyvacations.com\n",
      "42\n",
      "42\n",
      "Processing 43 of 61: https://stangerfabllc.com\n",
      "43\n",
      "43\n",
      "Processing 44 of 61: https://troychurch.net\n",
      "44\n",
      "44\n",
      "Processing 45 of 61: https://anthonymarlowe.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "Processing 46 of 61: https://avonnylocksmith.com\n",
      "46\n",
      "46\n",
      "Processing 47 of 61: https://exaintelligence.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "47\n",
      "Processing 48 of 61: https://mannexcavating.com\n",
      "48\n",
      "48\n",
      "Processing 49 of 61: https://miqrotech.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "49\n",
      "Processing 50 of 61: https://morrisholmes.com\n",
      "50\n",
      "50\n",
      "Processing 51 of 61: https://rfacapitalcorp.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "51\n",
      "Processing 52 of 61: https://ameecleaning.com\n",
      "52\n",
      "52\n",
      "Processing 53 of 61: https://cindywalker.com\n",
      "53\n",
      "53\n",
      "Processing 54 of 61: https://fivestarpoollinerswilliamstown.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "Processing 55 of 61: https://sustainableracine.org\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "55\n",
      "Processing 56 of 61: https://clearmirrorhealing.wordpress.com\n",
      "56\n",
      "56\n",
      "Processing 57 of 61: https://d-l-roofing-and-construction-llc.business.site\n",
      "57\n",
      "57\n",
      "Processing 58 of 61: https://lealvideoproductions.com\n",
      "58\n",
      "58\n",
      "Processing 59 of 61: https://rubpage.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "59\n",
      "Processing 60 of 61: https://sagacitystays.com\n",
      "60\n",
      "60\n",
      "Processing 61 of 61: https://valezio.com\n",
      "61\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "emailList = []\n",
    "mediaList = []\n",
    "phoneList = []\n",
    "addressList = []\n",
    "domainList = []\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Connection': 'keep-alive'\n",
    "    }\n",
    "\n",
    "def parseHTML(contactUrl, domain):\n",
    "    hrefs = []\n",
    "    emails = []\n",
    "    addressSites = []\n",
    "    phoneNumbers = []    \n",
    "    for url in contactUrl:\n",
    "        if url_pattern.match(url):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.encoding = response.apparent_encoding\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    # Find emails\n",
    "                    findEmails(soup, emails)\n",
    "                    # Find phone numbers\n",
    "                    findPhoneNumbers(soup, phoneNumbers)\n",
    "                    # Find social media links\n",
    "                    findLinks(soup, hrefs)\n",
    "                    # Find addresses\n",
    "                    findText(soup, addressSites)\n",
    "\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    # Find emails\n",
    "                    findEmails(soup, emails)\n",
    "                    # Find phone numbers\n",
    "                    findPhoneNumbers(soup, phoneNumbers)\n",
    "                    # Find social media links\n",
    "                    findLinks(soup, hrefs)\n",
    "                    # Find addresses\n",
    "                    findText(soup, addressSites)\n",
    "                else:\n",
    "                    continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    mediaList.append(list(hrefs) if hrefs else [''])\n",
    "    emailList.append(list(emails) if emails else [''])\n",
    "    phoneNumbers = list(set(phoneNumbers))  # Remove duplicates\n",
    "    phoneList.append(list(phoneNumbers) if phoneNumbers else [''])\n",
    "\n",
    "\n",
    "    maxLen = 0\n",
    "\n",
    "\n",
    "    if addressSites != [] and addressSites != [''] and addressSites != [['']] and addressSites != [[]]:\n",
    "\n",
    "        for address in addressSites:\n",
    "            if address != ['']:\n",
    "                if len(address[0]) > maxLen and len(address[0]) < 110:\n",
    "                    maxLen = len(address[0])\n",
    "                    addressFinal = address[0]\n",
    "            else:\n",
    "                addressFinal = None\n",
    "            \n",
    "        if addressFinal != None:      \n",
    "            addressList.append([addressFinal])\n",
    "        else:\n",
    "            addressList.append([''])\n",
    "    else:\n",
    "        addressList.append([''])\n",
    "        \n",
    "    domainList.append(domain)\n",
    "    \n",
    "i = 0\n",
    "df_websites = pd.read_csv(\"sample-websites.csv\")\n",
    "for index, row in df_websites.iterrows():\n",
    "    i += 1\n",
    "    domain = row['domain']\n",
    "    \n",
    "    if domain.startswith(\"http://\") or domain.startswith(\"https://\"):\n",
    "        website = domain\n",
    "    else:\n",
    "        website = \"https://\" + domain\n",
    "    print(f\"Processing {i} of {len(df_websites)}: {website}\")\n",
    "    \n",
    "    contactUrl = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(website, headers=headers)\n",
    "        response.encoding = response.apparent_encoding\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            contactUrl.append(website)\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find links that contain \"Contact\"\n",
    "            contact_links = soup.find_all('a', string=contact_pattern)\n",
    "            for link in contact_links:\n",
    "                if link not in contactUrl:\n",
    "                    contactUrl.append(link.get('href'))\n",
    "            # Find links that contain \"About\"\n",
    "            about_links = soup.find_all('a', string=about_pattern)\n",
    "            for link in about_links:\n",
    "                if link not in contactUrl:\n",
    "                    contactUrl.append(link.get('href'))\n",
    "\n",
    "            for url in contactUrl:\n",
    "                if url == None:\n",
    "                    contactUrl.remove(url)\n",
    "                else:\n",
    "                    cleaned_url = clean_url(url)\n",
    "                    if cleaned_url:\n",
    "                        if not url_pattern.match(cleaned_url):\n",
    "                            contactUrl.remove(url)\n",
    "                            contactUrl.append(website + cleaned_url)\n",
    "                    else:\n",
    "                        contactUrl.remove(url)\n",
    "\n",
    "            unique_contactUrl = list(set(contactUrl))\n",
    "            parseHTML(unique_contactUrl, row['domain'])\n",
    "        except Exception as e:\n",
    "            domainList.append(domain)\n",
    "            mediaList.append([''])\n",
    "            emailList.append([''])\n",
    "            phoneList.append([''])\n",
    "            addressList.append([''])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        domainList.append(domain)\n",
    "        mediaList.append([''])\n",
    "        emailList.append([''])\n",
    "        phoneList.append([''])\n",
    "        addressList.append([''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'Domain': np.array(domainList),\n",
    "    'Email': np.array(emailList_final),\n",
    "    'Media': np.array(mediaList_final),\n",
    "    'Phone': np.array(phoneList_final),\n",
    "    'Address': np.array(addressList) \n",
    "})\n",
    "\n",
    "\n",
    "df.to_csv(\"Submission_Test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
